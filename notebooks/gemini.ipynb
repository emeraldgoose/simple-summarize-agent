{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../secret.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://careersatdoordash.com/blog/how-doordash-leverages-llms-for-better-search-retrieval/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 1/1 [00:00<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "\n",
    "loader = AsyncHtmlLoader(url)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How DoorDash leverages LLMs for better search retrieval - DoorDash\n",
      "\n",
      "[Skip to content](#content)\n",
      "\n",
      "[![DoorDash](https://careersatdoordash.com/wp-content/themes/external-job-board/theme/images/doordash.png)](https://careersatdoordash.com/)\n",
      "\n",
      "* [Mission & Values](https://careersatdoordash.com/mission-and-values/)\n",
      "* [Working at DoorDash](https://careersatdoordash.com/working-at-doordash/)\n",
      "* [Belonging](https://careersatdoordash.com/belonging/)\n",
      "* [Blogs](https://careersatdoordash.com/blog/)\n",
      "  + [Career Blog](https://careersatdoordash.com/blog/)\n",
      "  + [Engineering Blog](https://careersatdoordash.com/engineering-blog/)\n",
      "* [Career Areas](https://careersatdoordash.com/career-areas/)\n",
      "  + [Engineering](https://careersatdoordash.com/career-areas/engineering/)\n",
      "  + [Design](https://careersatdoordash.com/career-areas/design/)\n",
      "  + [DashMart](https://careersatdoordash.com/career-areas/dashmart/)\n",
      "  + [Marketing](https://careersatdoordash.com/career-areas/marketing/)\n",
      "  + [Sales](https://careersatdoordash.com/career-areas/sales/)\n",
      "  + [Strategy & Operations](https://careersatdoordash.com/career-areas/strategy-operations/)\n",
      "  + [Product](https://careersatdoordash.com/career-areas/product/)\n",
      "  + [Analytics](https://careersatdoordash.com/career-areas/analytics/)\n",
      "  + [Policy and Communications](https://careersatdoordash.com/career-areas/policy-and-communications/)\n",
      "  + [IT](https://careersatdoordash.com/career-areas/it/)\n",
      "* [University Careers](https://careersatdoordash.com/university-careers/)\n",
      "* [Search Jobs](https://careersatdoordash.com/job-search/)\n",
      "\n",
      "Blog\n",
      "\n",
      "---\n",
      "\n",
      "# How DoorDash leverages LLMs for better search retrieval\n",
      "\n",
      "November 19, 2024\n",
      "\n",
      "|\n",
      "\n",
      "![Eduardo Martinez](https://careersatdoordash.com/wp-content/uploads/2024/11/ACBC8395-43D0-454C-984D-E9F0C6F1DB30-scaled.jpg)\n",
      "\n",
      "Eduardo Martinez\n",
      "\n",
      "![](https://careersatdoordash.com/wp-content/uploads/2024/11/kasia-derenda-1KDXAcyd7H0-unsplash-scaled-e1731978199415.jpg)\n",
      "\n",
      "At DoorDash, users commonly conduct searches using precise queries that compound multiple requirements. As a result, our search system has to be flexible enough to generalize well to novel queries while also allowing us to enforce specific rules to ensure search result quality.\n",
      "\n",
      "For instance, a query such as “*vegan chicken sandwich,*” for which a retrieval system that relies on document similarity — such as an embedding-based system —  could retrieve documents (i.e., items) such as:\n",
      "\n",
      "* Vegan sandwiches\n",
      "* Vegetarian sandwiches\n",
      "* Chicken sandwiches\n",
      "* Vegan chicken sandwiches\n",
      "\n",
      "For these keywords only the last set on that list matches the user intent exactly. But preferences may vary for different attributes. For instance, a consumer might be open to considering any vegan sandwich as an alternative but would reject a chicken sandwich that is not vegan; dietary restrictions often take precedence over other attributes, like protein choices. Several approaches could be used to show users only the most relevant results. At DoorDash, we believe a flexible hybrid system is most likely to meet our needs; a keyword-based retrieval system, combined with robust document and keyword understanding, can effectively enforce such rules as ensuring that only vegan items are retrieved. Here, we will detail how we have used large language models, or LLMs, to improve our retrieval system and give consumers more accurate search results.\n",
      "\n",
      "## Anatomy of a search engine\n",
      "\n",
      "Typical search engines contain different stages, which can be separated into two main journeys: one for documentsand another for queries. At DoorDash, documents refer to items or stores/restaurants, while queries are the search terms users enter into the search bar.\n",
      "\n",
      "![](https://careersatdoordash.com/wp-content/uploads/2024/11/image-1024x249.png)\n",
      "\n",
      "*Figure 1: Diagram of the life of a document and the life of a query.*\n",
      "\n",
      "As shown in Figure 1, the first step in a query's journey is to understand it. The query understanding module typically includes steps such as parsing and segmenting the query, annotating it with helpful information, linking it to specific concepts, and/or correcting spelling errors, among other stages. In our case, it also includes more specific steps, such as predicting the vertical intent of the query, whether the search is for a retailer/grocery item or a restaurant/food item.\n",
      "\n",
      "Similarly, on the document side, we have essential stages where we annotate and process documents with helpful information — metadata — before these are ingested into the search index and made available for retrieval. This information is leveraged not only for search use cases but also for other product surfaces, such as filters and analytical tools.\n",
      "\n",
      "### Document and query understanding\n",
      "\n",
      "At DoorDash, document processing relies in part on the knowledge graphs we have built for both food items and retail product items. These graphs allow us to define relationships between different entities, providing a better understanding of our documents.\n",
      "\n",
      "This means that stores and items contain rich metadata — tags and attributes — that help us understand our catalogs better. For example, for a retail item such as “Non-Dairy Milk & Cookies Vanilla Frozen Dessert - 8 oz,” we can have metadata that describes valuable information, including:\n",
      "\n",
      "* Dietary Preference: \"Dairy-free\"\n",
      "* Flavor: \"Vanilla\"\n",
      "* Product category: \"Ice cream\"\n",
      "* Quantity: “8 oz”\n",
      "\n",
      "We’ve previously written about how we’ve built DoorDash’s product knowledge graphs with LLMs; you can read more about that process [here](https://careers.doordash.com/blog/building-doordashs-product-knowledge-graph-with-large-language-models/).\n",
      "\n",
      "Queries can be segmented and then linked to the concepts available in our knowledge graphs. For example, a query like “small no-milk vanilla ice cream,\" can be segmented  to create chunks such as these:\n",
      "\n",
      "```\n",
      "[“small”, “no-milk”, “vanilla ice cream”]\n",
      "```\n",
      "\n",
      "We can then link each segment to attributes that are part of the metadata of the previous product. We might, however, find it difficult to link some of these segments to the precise attributes depending on the granularity of the segments; for “vanilla ice cream” we need to link to two different fields: the dish type “ice cream” and the flavor attribute “vanilla.” Our solution should be context aware to allow appropriate segmentation and entity linking.\n",
      "\n",
      "## LLMs for query understanding\n",
      "\n",
      "### Query segmentation\n",
      "\n",
      "Traditionally, query segmentation relies on methods such as pointwise mutual information (PMI) or n-gram analysis to determine which words in a query are likely to form meaningful word segments. These methods can be effective if the queries are relatively simple. They begin to fall short when dealing with complex queries that include multiple overlapping entities or when the queries have a high degree of ambiguity.\n",
      "\n",
      "For instance, in the query “turkey sandwich with cranberry sauce,” – is “cranberry sauce” a separate item or is it an attribute of the “sandwich”? Lacking context, traditional methods might struggle to capture relationships between these word segments.\n",
      "\n",
      "However, given the correct information, most modern LLMs can understand complex queries and provide accurate segmentations that consider word relationships within different contexts.\n",
      "\n",
      "One problem with LLMs, however, is that they are prone to hallucinations. We needed to develop a controlled vocabulary to create meaningful segmentations that are both factual and valuable for our retrieval system. Luckily, our knowledge graph work already offered an ontology that gave us access to multiple taxonomies that could guide this process. Instead of breaking down a search query into arbitrary segments, we prompt the model to identify meaningful segments and categorize them under our taxonomies. Even though the hallucination rate on the segmentation process is low — less than one percent — we also benefit from the immediate classification of the output in a valuable category for our retrieval system.\n",
      "\n",
      "We have taxonomies for restaurant items that define hierarchical relationships for cuisines, dish types, meal types, and dietary preferences, among many others. Similarly, we have taxonomies for retail items that include brands, dietary preferences, and product categories.\n",
      "\n",
      "As an example, let’s take another look at the previous query: “small no-milk vanilla ice cream.” Instead of asking the model simply to find meaningful word segments such as:\n",
      "\n",
      "```\n",
      "[“small”, “no-milk”, “vanilla ice cream”]\n",
      "```\n",
      "\n",
      "we prompt it to provide a structured output mapping each meaningful word segment to one of our taxonomy categories:\n",
      "\n",
      "```\n",
      "{\n",
      "\n",
      "Quantity: \"small\", \n",
      "\n",
      "Dietary_Preference: \"no-milk\", \n",
      "\n",
      "Flavor: \"vanilla\", \n",
      "\n",
      "Product_Category: \"ice cream\"\n",
      "\n",
      "}\n",
      "```\n",
      "\n",
      "Our evaluations have shown that this approach results in more accurate segmentations, likely because the structured categories provide the model with additional context about possible relationships.\n",
      "\n",
      "### Entity linking\n",
      "\n",
      "Once a query has been segmented, we want to map these segments to concepts available in our knowledge graph. Because the knowledge graph has been ingested into the search index as part of our document understanding work, we can make many rich attributes available for retrieval. A segment like “no-milk” should be linked to our “dairy-free” concept to ensure that we retrieve a candidate set that contains this attribute without restricting it to exact string matching in the item name or description, which can hurt recall.\n",
      "\n",
      "LLMs have proved very useful for this task as well. However, as we mentioned in the query segmentation section, they can sometimes generate outputs that are factually incorrect or hallucinated. In the context of entity linking, this could mean mapping a query segment to a concept that doesn't exist in our knowledge graph or mislabeling it entirely. To mitigate this, we employ techniques that constrain the model's output to include only concepts within our controlled vocabulary – in other words, our taxonomy concepts.\n",
      "\n",
      "We reduce these types of errors by providing the LLM with a curated list of candidate labels retrieved via approximate nearest neighbor (ANN) techniques. This approach ensures that the model selects from concepts that already are part of our knowledge graph, maintaining consistency and accuracy in the mapping.\n",
      "\n",
      "Consider the earlier query segment “no-milk,” for which our ANN retrieval system might provide candidate entities like “dairy-free” or “vegan.” The LLM then only needs to select the most appropriate concept based on the context, ensuring that the final mapping is accurate and within our knowledge graph.\n",
      "\n",
      "To do this, we leverage retrieval-augmented generation, or RAG. The process generally goes as follows:\n",
      "\n",
      "1. For each search query and knowledge graph taxonomy concept (candidate label), we produce embeddings. These can be from closed-source models, pre-trained, or learned in-house.\n",
      "2. Then, using an ANN retrieval system, we retrieve the closest 100 taxonomy concepts, or candidate labels, for each search query. We need to do this because of context window limitations and to reduce the noise in the prompt which can degrade performance (for details, see this [paper](https://arxiv.org/abs/2307.03172)).\n",
      "3. We then prompt the LLM to link queries to corresponding entities from specific taxonomies such as dish types, dietary preferences, cuisines, etc.\n",
      "\n",
      "This process ultimately generates a set of linked taxonomy concepts for each query that we can use directly to retrieve items from the search index. The overall process is outlined in Figure 2 below.\n",
      "\n",
      "![](https://careersatdoordash.com/wp-content/uploads/2024/11/Search-FKG-Entity-Linking-1024x1024.png)\n",
      "\n",
      "*Figure 2: Using LLMs for query segmentation and entity linking*\n",
      "\n",
      "After this process, the final query understanding signal for “small no-milk vanilla ice cream” would match with many of the attributes of the document, or item, in our catalog described as “Non-Dairy Milk & Cookies Vanilla Frozen Dessert - 8oz”:\n",
      "\n",
      "```\n",
      "{\n",
      "\n",
      "Dietary_Preference: \"Dairy-Free\", \n",
      "\n",
      "Flavor: \"Vanilla\", \n",
      "\n",
      "Product_Category: \"Ice cream\"\n",
      "\n",
      "}\n",
      "```\n",
      "\n",
      "This makes it easier to control what to retrieve by implementing a specific retrieval logic, such as making all dietary restrictions a *MUST* condition and allowing flexibility of less strict attributes such as flavors as a *SHOULD* condition.\n",
      "\n",
      "### Evaluations\n",
      "\n",
      "Maintaining high precision in our query understanding pipeline is crucial, especially when dealing with important attributes such as dietary preferences. To ensure this, we developed post-processing steps to prevent potential hallucinations in the final output and ensure the validity of both our segmented queries and their linked entities. After these post-processing steps, we perform manual audits on each batch of processed queries to measure the quality of our system.\n",
      "\n",
      "Annotators review a statistically significant sample of the output to verify that query segments are correctly identified and accurately linked to the appropriate entities in the knowledge graph. This manual evaluation helps us detect and correct systematic errors, refine prompts and processes, and maintain high precision.\n",
      "\n",
      "### Memorization vs. generalization trade-offs\n",
      "\n",
      "While our process shows that LLMs provide a good framework for query understanding, it’s important to keep in mind the trade-offs between memorization and generalization. Using LLMs for batch inference on a fixed set of queries can provide highly accurate results. This approach works well when the query space is limited and well-defined, but it becomes challenging as we move further into the long tail of the distribution.\n",
      "\n",
      "There are serious drawbacks to relying solely on memorization, including:\n",
      "\n",
      "* Scalability: As new queries emerge, especially in DoorDash’s dynamic environment, it becomes impractical to pre-process every possible query in a timely manner.\n",
      "* Maintenance: The system requires frequent updates and re-processing to incorporate new queries or changes in the knowledge graph.\n",
      "* Feature staleness: Some segmentations and links likely become stale over time.\n",
      "\n",
      "Fortunately, other methods generalize well to unseen queries, such as embedding retrieval, traditional statistical models, and other rule-based systems that can handle new queries on the fly. Such methods provide advantages such as:\n",
      "\n",
      "* Scalability: The ability to process any query without prior exposure.\n",
      "* Flexibility: Adaptation to evolving language usage and emerging trends.\n",
      "* Real-time processing: Immediate handling of queries without batch processing delays.\n",
      "\n",
      "As we mentioned, however, these methods may lack LLMs' deep contextual understanding, potentially reducing precision. A hybrid approach strikes the right balance between memorization and generalization. By combining the approach we outline here with other methods that generalize well to new query-document pairs – including lightweight heuristics, statistical methods such as BM25, or more complex approaches like embedding retrieval – we can leverage multiple strengths to achieve higher precision while maintaining adaptability.\n",
      "\n",
      "### System View: Integrating the new query understanding signal into the search pipeline\n",
      "\n",
      "The effectiveness of our query understanding system also depends on how well it integrates with other components of the search pipeline, particularly the rankers. Rankers are responsible for ordering the retrieved documents — items or stores — based on their relevance to the query.\n",
      "\n",
      "After introducing the new query understanding signals, we needed to make them available to the rankers. As the rankers caught up with the new signals and also the new patterns of consumer engagement that our retrieval improvements introduced, relevance and business metrics rose, as reflected in our online tests (see additional details below).\n",
      "\n",
      "By aligning the ranker's capabilities with the precision of our query understanding system, we are able to deliver more accurate and relevant search results. This synergy is essential to meet our users’ evolving and complex needs, as demonstrated in the following use case.\n",
      "\n",
      "## Results and a use case\n",
      "\n",
      "DoorDash’s popular dish carousel, shown in Figure 3, relies on this retrieval pipeline to display relevant results for queries that reflect a specific dish intent.\n",
      "\n",
      "![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdtCHiY5aoVhq85jnnBkzslLaWSibCVNNtXUT2dp74-yYh2-I0k2QQJquTg4YPKRJpGlX0EkYXkRcTacR5UHuVgs1BXhzSRXHasl4jIYlytDpi7ssfLRC5SZE580_gCt6eQk3ryOJz1WyhUD5L_-JYVQhRn?key=Emdn_dVkP7-sr0acaE6MGQ)\n",
      "\n",
      "*Figure 3: Ranked list of food items in the “Popular Dishes” carousel.*\n",
      "\n",
      "When consumers search for something like “açaí bowl,” the example shown in Figure 3, they signal that they are looking for a particular dish. By providing that specific dish directly in the search results page, they can quickly compare different options across many stores.\n",
      "\n",
      "We saw a substantial increase in the trigger rate of popular dish carousels upon implementation of our new query understanding and retrieval improvements–we are able to retrieve significantly more items. Specifically, we observed nearly a 30% increase over our baseline, which also means we are aligning search results more closely with consumer intent, making it easier for them to place orders.\n",
      "\n",
      "This increase in trigger rate should lead to more relevant results for consumers. When we accurately segment queries and link them to our knowledge graph, we can retrieve a broader and more precise set of dish items to populate these carousels. A higher trigger rate coupled with high-quality results means that we increase overall relevance. This is shown by our whole page relevance, or WPR, metric, which is designed to measure from the user’s perspective the overall relevance of search results across different query segments and intents. Our approach led to a more than two percent increase in WPR for dish-intent queries, indicating that users were seeing more relevant dishes in general.\n",
      "\n",
      "Online testing also showed that increased relevance aligns with an increase in engagement and conversion. We observed a rise in same-day conversions, confirming that reducing friction can help consumers decide which items to order.\n",
      "\n",
      "Furthermore, with new and more diverse engagement coming in from the improved retrieval systems, we could retrain our ranker with a more comprehensive dataset. The new ranker version further improved relevance — as demonstrated by a 1.6% increase in WPR — making it even easier for consumers to discover and order the dishes they wanted, resulting in higher order volume and increasing marketplace value.\n",
      "\n",
      "## Future directions\n",
      "\n",
      "Now that we have validated how well LLMs can be integrated into the DoorDash system, we have revealed a vast landscape of possibilities to explore. As we continue to automate processes to increase our query and catalog understanding, we can scale up the number of concepts and attributes identified in our catalogs and better understand the relationships between even more entities. Among the many use cases this can unlock are:\n",
      "\n",
      "* Helping users rewrite queries and recommending search paths they can explore. Our greater understanding of relationships enables us to suggest alternative or related search paths to guide users to new dishes, stores, and restaurants.\n",
      "* Showing new users which queries they may want to search because we can identify the most popular items in a given market to a high degree of granularity.\n",
      "* Improving retrieval recall and precision through better coverage of query and document understanding. More granular attributes allow us to retrieve more items without significantly compromising precision.\n",
      "* Learning more about consumer behavior and profiles. Deeper query and catalog understanding let us better understand the overlap of attributes between entities and create personalization signals that, for example, infer that a consumer likes spicy dishes and Latin American cuisines.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Through combining LLMs for query understanding with our knowledge graph and a flexible retrieval approach, we now can handle more complex and nuanced user queries while unlocking new experiences in a highly dynamic environment. We are excited to continue experimenting with new and emerging technologies, working with our partners to create delightful experiences for our consumers.\n",
      "\n",
      "### Acknowledgments\n",
      "\n",
      "Special thanks to Ran He, Paul Kim, Emily Strouse, Luoming Liu, Nishan Subedi, Tian Wang, Qilin Qi, Hanning Zhou, Yu Sun, Ying Li, Fushan Li, and the rest of the Knowledge Graph and Search Quality teams; to our cross-functional partners Christian Lai, Alex Levy, Kunal Moudgil, and Jose Grimaldo, who all worked together to make this exciting work happen. Finally, huge thanks to our team of annotators, who contribute significantly to our ML work.\n",
      "\n",
      "## About the Author\n",
      "\n",
      "* ![](https://careersatdoordash.com/wp-content/uploads/2024/11/ACBC8395-43D0-454C-984D-E9F0C6F1DB30-scaled.jpg)\n",
      "\n",
      "  Eduardo is a Machine Learning Engineer on the Consumer Search team at DoorDash where he has been working on Search and Knowledge Graphs for store and restaurant items.\n",
      "\n",
      "## Related Jobs\n",
      "\n",
      "[View All Jobs](https://careersatdoordash.com/job-search)\n",
      "\n",
      "[Quality Assurance & Verification Lead - Autonomy & Robotics](https://careersatdoordash.com/jobs/quality-assurance-verification-lead---autonomy-robotics/6694666)\n",
      "\n",
      "Job ID: 3074235\n",
      "\n",
      "Location\n",
      "\n",
      "San Francisco, CA\n",
      "\n",
      "Department\n",
      "\n",
      "Engineering\n",
      "\n",
      "[View Job](https://careersatdoordash.com/jobs/quality-assurance-verification-lead---autonomy-robotics/6694666)\n",
      "\n",
      "[Software Engineer, Developer Console](https://careersatdoordash.com/jobs/software-engineer-developer-console/6687390)\n",
      "\n",
      "Job ID: 3090069\n",
      "\n",
      "Location\n",
      "\n",
      "Los Angeles, CA; New York, NY; San Francisco, CA; Seattle, WA\n",
      "\n",
      "Department\n",
      "\n",
      "Engineering\n",
      "\n",
      "[View Job](https://careersatdoordash.com/jobs/software-engineer-developer-console/6687390)\n",
      "\n",
      "[Corporate Security Engineer](https://careersatdoordash.com/jobs/corporate-security-engineer/6679401)\n",
      "\n",
      "Job ID: 3087697\n",
      "\n",
      "Location\n",
      "\n",
      "United States - Remote\n",
      "\n",
      "Department\n",
      "\n",
      "Engineering\n",
      "\n",
      "[View Job](https://careersatdoordash.com/jobs/corporate-security-engineer/6679401)\n",
      "\n",
      "[Software Engineer, Android](https://careersatdoordash.com/jobs/software-engineer-android/6679608)\n",
      "\n",
      "Job ID: 3087762\n",
      "\n",
      "Location\n",
      "\n",
      "Pune, India\n",
      "\n",
      "Department\n",
      "\n",
      "Engineering\n",
      "\n",
      "[View Job](https://careersatdoordash.com/jobs/software-engineer-android/6679608)\n",
      "\n",
      "[Engineering Manager, Post-Checkout Experience](https://careersatdoordash.com/jobs/engineering-manager-post-checkout-experience/6686265)\n",
      "\n",
      "Job ID: 2728459\n",
      "\n",
      "Location\n",
      "\n",
      "San Francisco, CA; Sunnyvale, CA; Seattle, WA\n",
      "\n",
      "Department\n",
      "\n",
      "Engineering\n",
      "\n",
      "[View Job](https://careersatdoordash.com/jobs/engineering-manager-post-checkout-experience/6686265)\n",
      "\n",
      "## Recent Blogs\n",
      "\n",
      "[View All Blogs](https://careersatdoordash.com/blog)\n",
      "\n",
      "[culture\n",
      "\n",
      "DoorDash\n",
      "\n",
      "### From Parenthood to Productivity: How DoorDash supported my maternity journey\n",
      "\n",
      "Balancing a demanding tech career while becoming a new mom is no easy feat, but DoorDash’s support every step of the way made it possible for me to navigate this journey and thrive.](https://careersatdoordash.com/blog/doordash-supported-my-maternity-journey/)\n",
      "\n",
      "[culture\n",
      "\n",
      "DoorDash\n",
      "\n",
      "### How we’re celebrating Indigenous Heritage Month at DoorDash\n",
      "\n",
      "Throughout our month-long series of virtual and in-person events, we’re acknowledging all worldwide Indigenous members of our company and communities by celebrating the rich, resilient, and diverse cultures, traditions, and histories of Indigenous People, in the US, and globally.](https://careersatdoordash.com/blog/indigenous-heritage-month-doordash-2024/)\n",
      "\n",
      "[culture\n",
      "\n",
      "strategy-operations\n",
      "\n",
      "### How Strategy & Operations drives the DoorDash business forward\n",
      "\n",
      "At DoorDash, our Strategy & Operations (S&O) function is the engine that drives our business forward, supporting launching new geographies and products while building and testing the strategic plans at every step.](https://careersatdoordash.com/blog/strategy-operations-doordash-2024/)\n",
      "\n",
      "![DoorDash](https://careersatdoordash.com/wp-content/themes/external-job-board/theme/images/doordash-white.png)\n",
      "\n",
      "* [Careers Home](/)\n",
      "* [Mission & Values](https://careersatdoordash.com/mission-and-values/)\n",
      "* [Working at DoorDash](https://careersatdoordash.com/working-at-doordash/)\n",
      "* [Belonging](https://careersatdoordash.com/belonging/)\n",
      "* [Career Areas](https://careersatdoordash.com/career-areas/)\n",
      "* [University Careers](https://careersatdoordash.com/university-careers/)\n",
      "* [Career Blog](https://careersatdoordash.com/blog/)\n",
      "* [Talent Network](https://www.gem.com/form?formID=49e87e7d-3ed2-4d15-911c-58158e4a516e)\n",
      "* [Search Jobs](https://careersatdoordash.com/job-search/)\n",
      "\n",
      "Statement of Non-Discrimination: In keeping with our beliefs and goals, no employee or applicant will face discrimination or harassment based on: race, color, ancestry, national origin, religion, age, gender, marital/domestic partner status, sexual orientation, gender identity or expression, disability status, or veteran status. Above and beyond discrimination and harassment based on “protected categories,” we also strive to prevent other subtler forms of inappropriate behavior (i.e., stereotyping) from ever gaining a foothold in our office. Whether blatant or hidden, barriers to success have no place at DoorDash. We value a diverse workforce – people who identify as women, nonbinary or gender non-conforming, LGBTQIA+, American Indian or Native Alaskan, Black or African American, Hispanic or Latinx, Native Hawaiian or Other Pacific Islander, differently-abled, caretakers and parents, and veterans are strongly encouraged to apply. Thank you to the Level Playing Field Institute for this statement of non-discrimination.\n",
      "\n",
      "[Terms of Service](https://help.doordash.com/consumers/s/terms-and-conditions-us?language=en_US)\n",
      "[Consumer Privacy](https://help.doordash.com/consumers/s/privacy-policy-us?language=en_US)\n",
      "[Applicant Privacy Notice](https://help.doordash.com/legal/document?type=ax-privacy-notice®ion=US&locale=en-US)\n",
      "[Do Not Sell or Share My Personal Information](https://www.doordash.com/consumer/personalize)\n",
      "\n",
      "© 2025 DoorDash\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_transformers import MarkdownifyTransformer\n",
    "\n",
    "md = MarkdownifyTransformer()\n",
    "converted_docs = md.transform_documents(docs)\n",
    "\n",
    "print(converted_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How DoorDash leverages LLMs for better search retrieval - DoorDash'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = docs[0].metadata['title']\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-2.0-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "\n",
    "class MarkdownOutputParser(BaseOutputParser):\n",
    "    def parse(self, output: str) -> str:\n",
    "        cleaned_text = output.strip()\n",
    "        if len(cleaned_text) > 0:\n",
    "            pattern = r\"^\\s*```(?:\\w+)?\\n([\\s\\S]*?)\\n```\\s*$\"\n",
    "            match = re.match(pattern, cleaned_text)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "        else:\n",
    "            raise OutputParserException(\"Output is empty\")\n",
    "    \n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        return \"markdown_output_parseer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['document'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are helpful assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['document'], input_types={}, partial_variables={}, template='\\nYou should extract the keywords you find important from the document between the document xml tags. Output should be a list of keywords.\\n<document>{document}</document>\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_template = \"\"\"You are helpful assistant.\"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "You should extract the keywords you find important from the document between the document xml tags. Output should be a list of keywords.\n",
    "<document>{document}</document>\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.md', 'r') as f:\n",
    "    document = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_args = {\n",
    "    'document': document,\n",
    "}\n",
    "\n",
    "response = chain.invoke(invoke_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- DNN models\n",
      "- online performance\n",
      "- offline performance\n",
      "- DoorDash\n",
      "- Ads ML team\n",
      "- ranking models\n",
      "- debugging framework\n",
      "- scalable methodology\n",
      "- feature serving consistency\n",
      "- feature freshness\n",
      "- real-time features\n",
      "- model serving\n",
      "- model architecture\n",
      "- Multi-Task Multi-Label (MTML)\n",
      "- Feature Generation Disparity\n",
      "- Data Distribution Shift (Concept Drift)\n",
      "- Model Serving Instability\n",
      "- offline replay\n",
      "- AUC benchmark\n",
      "- Feature staleness\n",
      "- Cached residuals\n",
      "- Online Feature Store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keywords.md\", \"w\") as f:\n",
    "    f.write(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Figures, Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['document'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are helpful assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['document'], input_types={}, partial_variables={}, template='\\nYou should extract the figures and tables from the document.\\ndocument:\\n{document}\\nUse xml syntax to format your output:\\n- Use <img src=url> to insert an image. Provide a caption for the image with <caption> tag for accessibility.\\n- Use <table> for alignment. Keep tables concise and structured.\\nProvide the figures with <figures> tag and provide the tables with <tables> tag. If there no any images or tables, include empty figures or tables tag.\\nDo not output any other content.\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_template = \"\"\"You are helpful assistant.\"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "You should extract the figures and tables from the document.\n",
    "document:\n",
    "{document}\n",
    "Use xml syntax to format your output:\n",
    "- Use <img src=url> to insert an image. Provide a caption for the image with <caption> tag for accessibility.\n",
    "- Use <table> for alignment. Keep tables concise and structured.\n",
    "Provide the figures with <figures> tag and provide the tables with <tables> tag. If there no any images or tables, include empty figures or tables tag.\n",
    "Do not output any other content.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_args = {\n",
    "    'document': document,\n",
    "}\n",
    "\n",
    "response = chain.invoke(invoke_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```xml\n",
      "<figures>\n",
      "  <figure>\n",
      "    <img src=\"https://careersatdoordash.com/wp-content/uploads/2024/11/kasia-derenda-1KDXAcyd7H0-unsplash-scaled-e1731978199415.jpg\">\n",
      "    <caption>Featured image for the blog post.</caption>\n",
      "  </figure>\n",
      "  <figure>\n",
      "    <img src=\"https://careersatdoordash.com/wp-content/uploads/2024/11/image-1024x249.png\">\n",
      "    <caption>Figure 1: Diagram of the life of a document and the life of a query.</caption>\n",
      "  </figure>\n",
      "  <figure>\n",
      "    <img src=\"https://careersatdoordash.com/wp-content/uploads/2024/11/Search-FKG-Entity-Linking-1024x1024.png\">\n",
      "    <caption>Figure 2: Using LLMs for query segmentation and entity linking</caption>\n",
      "  </figure>\n",
      "    <figure>\n",
      "    <img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdtCHiY5aoVhq85jnnBkzslLaWSibCVNNtXUT2dp74-yYh2-I0k2QQJquTg4YPKRJpGlX0EkYXkRcTacR5UHuVgs1BXhzSRXHasl4jIYlytDpi7ssfLRC5SZE580_gCt6eQk3ryOJz1WyhUD5L_-JYVQhRn?key=Emdn_dVkP7-sr0acaE6MGQ\">\n",
      "    <caption>Figure 3: Ranked list of food items in the “Popular Dishes” carousel.</caption>\n",
      "  </figure>\n",
      "</figures>\n",
      "<tables>\n",
      "</table>\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = \"\"\"```xml\n",
    "<figures>\n",
    "<figure class=\"wp-block-image aligncenter is-resized\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdGouL9pDnn3LKih2h4mmEwcZGPIlQYOxX_M3itKMtBWq56qBttGbcEVh8MggWkFAZWSan6jJolWjFi9-nAENu0UFG0gIVKhz_5T6wWWokIpa0JrPpXj-K2xZN0Y1xYbMLL9VSV?key=2JKv1fZGhwooiun5eP7Ez3ot\" alt=\"\" style=\"width:960px;height:auto\"/><figcaption class=\"wp-element-caption\"><em>Figure 1: Restaurant Discovery Ads Ranking Deep Learning Milestones</em></figcaption></figure>\n",
    "<figure class=\"wp-block-image aligncenter is-resized\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXeC8PfTbKQ_oN14mOK-aGt2JZczRZjb0ivbKiS_XkeAc-CTFuMLh99iQAVOlXNTTRtdp3rPB15k9jg43-_qJiJNrj2M8bNLBvBtIi256_LyR7_Zq6e5zddxZIo2tFBQqRBLqsf4?key=2JKv1fZGhwooiun5eP7Ez3ot\" alt=\"\" style=\"width:746px;height:auto\"/><figcaption class=\"wp-element-caption\"><em>Figure 2: Feature Distribution Online (red) vs Offline (blue)</em></figcaption></figure>\n",
    "<figure class=\"wp-block-image aligncenter is-resized\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXfVu9h5JPpG3wnNU3x6zVw8rVqxankDAxsNyD2_QZlc-bn4gYjJtefjkV4pe0M58WsheHYsqgDNL_EEBHBdzzAFwH_M5CmumrYhRJJify_L0vzlXpm898hLl0EV4Yvf0MGfwSxarw?key=2JKv1fZGhwooiun5eP7Ez3ot\" alt=\"\" style=\"width:593px;height:auto\"/></figure>\n",
    "<figure class=\"wp-block-image aligncenter is-resized\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXcr0SIZWL1hl1VF1CBw29l9xYdHISxI6l3lwM0afEWGBLVcO4GCuVJBoZnwgko8VWzlTgFQUjxk6C6fhbPQtw4zj-NjCCHMZe6QbykCDs6UBWOFnBeTXhg0yp_yl3j2_igkny33ZQ?key=2JKv1fZGhwooiun5eP7Ez3ot\" alt=\"\" style=\"width:589px;height:auto\"/><figcaption class=\"wp-element-caption\"><em>Figure 3: Feature Staleness</em></figcaption></figure>\n",
    "<figure class=\"wp-block-image aligncenter size-large is-resized\"><img src=\"https://careersatdoordash.com/wp-content/uploads/2024/12/image-1024x565.png\" alt=\"\" class=\"wp-image-13500\" style=\"width:626px;height:auto\"/><figcaption class=\"wp-element-caption\"><em>Figure 4: AUC Relative Changes on models trained by -1/2/3/4 day feature offsets</em></figcaption></figure>\n",
    "</figures>\n",
    "\n",
    "<tables>\n",
    "<figure class=\"wp-block-table\"><table class=\"has-fixed-layout\"><tbody><tr><td><strong>Feature Category</strong></td><td><strong>Feature Data Type</strong></td><td><strong>Description</strong></td></tr><tr><td rowspan=\"2\">Existing Features</td><td>Dense Features</td><td>Mostly consumer engagement features</td></tr><tr><td>Sequence Features</td><td>Consumer-engaged business/food/cuisine tag sequence &amp; contextual features</td></tr><tr><td>Newly Added Features</td><td>Dense Features</td><td>Consumer promotion-related featuresAdditional consumer engagement features</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 1: Model for investigation Feature details</em></figcaption></figure>\n",
    "<figure class=\"wp-block-table\"><table class=\"has-fixed-layout\"><thead><tr><th><strong>Eval Date Range</strong></th><th><strong>Model</strong></th><th><strong>Feature Generation</strong></th><th><strong>Online vs Offline</strong></th><th><strong>AUC</strong></th></tr></thead><tbody><tr><td rowspan=\"3\">One Week’s data in September</td><td>Baseline (Prod Model)</td><td>Online Logging</td><td>Online</td><td>Baseline Value</td></tr><tr><td>New model</td><td>Online Logging</td><td>Online Shadow</td><td>-1.80%</td></tr><tr><td>New model</td><td>-1d offline join new added features</td><td>Offline Replay</td><td>+2.05%</td></tr><tr><td rowspan=\"2\">One Week’s data in June</td><td>Baseline (Prod Model)</td><td>Online Logging</td><td>Offline</td><td>+0.77%</td></tr><tr><td>New model</td><td>-1d offline join new added features</td><td>Offline</td><td>+2.105%</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 2: AUC Benchmarks for offline Reply</em></figcaption></figure>\n",
    "<figure class=\"wp-block-table\"><table class=\"has-fixed-layout\"><tbody><tr><td><strong>Feature Name</strong></td><td><strong>Feature Aggregation Level</strong></td><td><strong>Feature Aggregation Time Window</strong></td><td><strong>Feature Staleness</strong></td><td><strong>Feature Missing Rate</strong></td><td><strong>% of cached residuals</strong></td></tr><tr><td>Feature 1</td><td>Consumer level</td><td>Past 1 year</td><td>-3d/-4d</td><td>2.77%</td><td>1.15%</td></tr><tr><td>Feature 2</td><td>&lt;Consumer, Store&gt; level</td><td>Past 3 month</td><td>-3d</td><td>76.20%</td><td>45.6%</td></tr><tr><td>Feature 3</td><td>&lt;Consumer, Store&gt; level</td><td>Past half-year</td><td>-3d</td><td>70.19%</td><td>23.6%</td></tr><tr><td>Feature 4</td><td>Consumer level</td><td>Past 1 year</td><td>-3d/-4d</td><td>2.77%</td><td>6.07%</td></tr><tr><td>Feature 5</td><td>Consumer level</td><td>Past 3 months</td><td>-3d</td><td>45.18%</td><td>4.56%</td></tr><tr><td>Feature 6</td><td>Consumer level</td><td>Past 3 months</td><td>-3d</td><td>76.19%</td><td>31.0%</td></tr><tr><td>Feature 7</td><td>Consumer level</td><td>Past 1 month</td><td>-2d</td><td>76.19%</td><td>31.0%</td></tr><tr><td>Feature 8</td><td>Store level</td><td>Past 3 months</td><td>-3d</td><td>0.92%</td><td>34.9%</td></tr><tr><td>Feature 9</td><td>Consumer level</td><td>Past 3 months</td><td>-2d</td><td>45.18%</td><td>4.55%</td></tr><tr><td>Feature 10</td><td>Store level</td><td>Past 1 day</td><td>-3d</td><td>23.50%</td><td>24.7%</td></tr></tbody></table><figcaption class=\"wp-element-caption\">Table 3: Cache Residuals of the 10 most important added features</figcaption></figure>\n",
    "<figure class=\"wp-block-table\"><table class=\"has-fixed-layout\"><tbody><tr><td><strong>Feature Name</strong></td><td><strong>Feature Aggregation Level</strong></td><td><strong>Feature Aggregation Time Window</strong></td><td><strong>% of entities change**</strong></td><td><strong>% of feature value mismatch***</strong></td></tr><tr><td>Feature 1</td><td>Consumer level</td><td>Past 1 year</td><td>0.0258%</td><td>9.69%</td></tr><tr><td>Feature 2</td><td>&lt;Consumer, Store&gt; level</td><td>Past 3 month</td><td>1.04%</td><td>5.2%</td></tr><tr><td>Feature 3</td><td>&lt;Consumer, Store&gt; level</td><td>Past half-year</td><td>3.36%</td><td>11.3%</td></tr><tr><td>Feature 4</td><td>Consumer level</td><td>Past 1 year</td><td>0.0258%</td><td>9.68%</td></tr><tr><td>Feature 5</td><td>Consumer level</td><td>Past 3 months</td><td>1.0%</td><td>8.37%</td></tr><tr><td>Feature 6</td><td>Consumer level</td><td>Past 3 months</td><td>1.04%</td><td>4.66%</td></tr><tr><td>Feature 7</td><td>Consumer level</td><td>Past 1 month</td><td>1.04%</td><td>4.66%</td></tr><tr><td>Feature 8</td><td>Store level</td><td>Past 3 months</td><td>0.09%</td><td>4.38%</td></tr><tr><td>Feature 9</td><td>Consumer level</td><td>Past 3 months</td><td>1.04%</td><td>4.78%</td></tr><tr><td>Feature 10</td><td>Store level</td><td>Past 1 day</td><td>0.436%</td><td>35.7%</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 4: Cache Residuals of the 10 most important added features<br>**<strong> </strong>the percentage of entity_ids that did not show up in the previous data.<br>***the percentage of feature values that are different from the previous day.&nbsp;</em></figcaption></figure>\n",
    "<figure class=\"wp-block-table\"><table class=\"has-fixed-layout\"><tbody><tr><td>Solution</td><td>Pros</td><td>Cons</td></tr><tr><td>Short-term</td><td>Reduces AUC discrepancy immediately</td><td>Does not address cached residuals</td></tr><tr><td>Long-term</td><td>Effectively resolves both cached residuals and feature staleness.Improves model generalization.</td><td>Has the trade-off between development speed and data accuracyRequires system stability improvements to support feature logging of larger traffic.</td></tr></tbody></table><figcaption class=\"wp-element-caption\"><em>Table 5: Comparison between short-term and long-term solutions</em></figcaption></figure>\n",
    "</tables>\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "figures_and_tables = MarkdownOutputParser().parse(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "figures = re.search(r'<figures>(.*?)</figures>', figures_and_tables,re.DOTALL).group(1).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = re.search(r'<tables>(.*?)</tables>', figures_and_tables,re.DOTALL).group(1).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['document', 'figures', 'keywords', 'language', 'tables'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are helpful assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['document', 'figures', 'keywords', 'language', 'tables'], input_types={}, partial_variables={}, template='\\nYou should create a summary from the document between the document xml tags that corresponds to the keywords between the keywords xml tags, but be sure to include important information and place any relevant figures or tables alongside it.\\nWrite in {language}.\\n<document>{document}</document>\\n<keywords>{keywords}</keywords>\\n<figures>{figures}</figures>\\n<tables>{tables}</tables>\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_template = \"\"\"You are helpful assistant.\"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "You should create a summary from the document between the document xml tags that corresponds to the keywords between the keywords xml tags, but be sure to include important information and place any relevant figures or tables alongside it.\n",
    "Write in {language}.\n",
    "<document>{document}</document>\n",
    "<keywords>{keywords}</keywords>\n",
    "<figures>{figures}</figures>\n",
    "<tables>{tables}</tables>\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('keywords.md', 'r') as f:\n",
    "    keywords = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_args = {\n",
    "    'document': document,\n",
    "    'keywords': keywords,\n",
    "    'figures': figures,\n",
    "    'tables': tables,\n",
    "    'language': 'Korean',\n",
    "}\n",
    "\n",
    "response = chain.invoke(invoke_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summary.md\", \"w\") as f:\n",
    "    f.write(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Reflect #####\n",
      "reflection=Reflection(missing=\"검색 스택의 격리 구조가 실제 문제 발생 시 어떤 이점을 제공하는지에 대한 구체적인 사례가 부족합니다. 예를 들어, 특정 검색 스택에서 장애가 발생했을 때 다른 스택은 어떻게 영향을 받지 않는지 설명하는 것이 필요합니다. 또한, '제어 평면'의 역할과 기능에 대한 추가 설명이 필요합니다.\", advisable=\"문장이 전반적으로 훌륭하지만, '검색 스택'과 '제어 평면'이라는 핵심 용어에 대한 더 자세한 설명이나 예시를 추가하면 독자의 이해도를 높일 수 있습니다. 예를 들어, '검색 스택'이 실제로 어떻게 데이터와 트래픽을 격리하는지, 그리고 '제어 평면'이 구체적으로 어떤 기능을 수행하여 유연성을 확보하는지 설명하는 것이 좋습니다.\", superfluous='문장이 전반적으로 간결하고 핵심 내용을 잘 전달하고 있으므로 불필요한 부분은 없습니다.') search_queries=['DoorDash 검색 스택 장애 사례', 'DoorDash 제어 평면 아키텍처', '검색 스택 격리 효과']\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Reflection(BaseModel):\n",
    "    missing: str = Field(description=\"Critique of what is missing\")\n",
    "    advisable: str = Field(description=\"Critique of what is helpful for better writing\")\n",
    "    superfluous: str = Field(description=\"Critique of what is superfluous\")\n",
    "\n",
    "class Research(BaseModel):\n",
    "    \"\"\"Provide reflection and then follow up with search queries to improve the writing.\"\"\"\n",
    "\n",
    "    reflection: Reflection = Field(description=\"Your reflection on the initial writing for summary.\")\n",
    "    search_queries: List[str] = Field(description=\"1-3 search queries for researching improvements to address the critique of your current writing.\")\n",
    "\n",
    "print('##### Reflect #####')\n",
    "draft = \"\"\"일반적인 목적의 검색 엔진을 사용하는 고객에게 데이터와 트래픽을 격리하는 것은 매우 중요한 요소이다. 이를 위해 DoorDash는 **검색 스택(Search Stack)**이라는 개념을 도입했다. 검색 스택은 특정 인덱스에 전념하는 검색 서비스의 모음으로, 하나의 검색 스택에서 발생한 문제가 다른 검색 스택에 영향을 미치지 않도록 설계되었다. 즉, 각 검색 스택은 독립적으로 운영되며, 특정 인덱스에 대한 빌드 및 쿼리 작업만 수행할 수 있다. 이러한 격리 구조는 한 검색 스택에서 예기치 않은 문제가 발생하더라도 다른 검색 스택의 서비스에는 영향을 주지 않도록 보장한다. 또한, 각 테넌트가 프로비저닝한 모든 리소스를 쉽게 파악하고 관리할 수 있도록 지원하여, 리소스 사용에 대한 투명성을 확보하고 책임 소재를 명확히 할 수 있다. 검색 스택은 테넌트의 인덱스 스키마와 서비스를 격리하는 데 매우 유용하며, 스키마 및 스택 구성 변경 시 이전 버전과의 호환성을 고려하지 않고도 손쉽게 변경할 수 있다는 장점을 제공한다. 이러한 유연성을 확보하기 위해 DoorDash는 **제어 평면(Control Plane)**이라는 오케스트레이션 서비스를 설계했다.\"\"\"\n",
    "\n",
    "reflection = []\n",
    "search_queries = []\n",
    "for _ in range(2):\n",
    "    structured_llm = llm.with_structured_output(Research, include_raw=True)\n",
    "    info = structured_llm.invoke(draft)\n",
    "    if not info['parsed'] == None:\n",
    "        parsed_info = info['parsed']\n",
    "        print(parsed_info)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색 스택의 격리 구조가 실제 문제 발생 시 어떤 이점을 제공하는지에 대한 구체적인 사례가 부족합니다. 예를 들어, 특정 검색 스택에서 장애가 발생했을 때 다른 스택은 어떻게 영향을 받지 않는지 설명하는 것이 필요합니다. 또한, '제어 평면'의 역할과 기능에 대한 추가 설명이 필요합니다.\n",
      "문장이 전반적으로 훌륭하지만, '검색 스택'과 '제어 평면'이라는 핵심 용어에 대한 더 자세한 설명이나 예시를 추가하면 독자의 이해도를 높일 수 있습니다. 예를 들어, '검색 스택'이 실제로 어떻게 데이터와 트래픽을 격리하는지, 그리고 '제어 평면'이 구체적으로 어떤 기능을 수행하여 유연성을 확보하는지 설명하는 것이 좋습니다.\n",
      "문장이 전반적으로 간결하고 핵심 내용을 잘 전달하고 있으므로 불필요한 부분은 없습니다.\n"
     ]
    }
   ],
   "source": [
    "print(parsed_info.reflection.missing)\n",
    "print(parsed_info.reflection.advisable)\n",
    "print(parsed_info.reflection.superfluous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['figures', 'keywords', 'summary', 'tables'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a technical writer. You have been given a document to review. You need to make sure that the document is well-written and that the reader can understand it. If there is a problem, you should put the answer “Fail” in the answer xml tag and the problem in the error xml tag. If it passes, you should put the answer “Pass” in the answer xml tag and the problem in the error xml tag. Output should be respective xml tags.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['figures', 'keywords', 'summary', 'tables'], input_types={}, partial_variables={}, template='\\nHere is the summary of the document:\\n<summary>{summary}</summary>\\n\\nHere are the keywords extracted from the document:\\n<keywords>{keywords}</keywords>\\n\\n<figures>{figures}</figures>\\n\\n<tables>{tables}</tables>\\n\\nPlease review the document and provide feedback.\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_template = \"\"\"You are a technical writer. You have been given a document to review. You need to make sure that the document is well-written and that the reader can understand it. If there is a problem, you should put the answer “Fail” in the answer xml tag and the problem in the error xml tag. If it passes, you should put the answer “Pass” in the answer xml tag and the problem in the error xml tag. Output should be respective xml tags.\"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "Here is the summary of the document:\n",
    "<summary>{summary}</summary>\n",
    "\n",
    "Here are the keywords extracted from the document:\n",
    "<keywords>{keywords}</keywords>\n",
    "\n",
    "<figures>{figures}</figures>\n",
    "\n",
    "<tables>{tables}</tables>\n",
    "\n",
    "Please review the document and provide feedback.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summary.md\", \"r\") as f:\n",
    "    summary = f.read()\n",
    "\n",
    "with open(\"keywords.md\", \"r\") as f:\n",
    "    keywords = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_args = {\n",
    "    'summary': summary,\n",
    "    'keywords': keywords,\n",
    "    'figures': figures,\n",
    "    'tables': tables,\n",
    "}\n",
    "\n",
    "response = chain.invoke(invoke_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```xml\n",
      "<answer>Pass</answer>\n",
      "<error>No errors found. The document is well-written, clearly explains the problem, the methodology used to identify the root cause, and the proposed solutions. The use of figures and tables is effective in presenting data and results. The summary is also accurate and concise.</error>\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<answer>Pass</answer>\\n<error>No errors found. The document is well-written, clearly explains the problem, the methodology used to identify the root cause, and the proposed solutions. The use of figures and tables is effective in presenting data and results. The summary is also accurate and concise.</error>'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MarkdownOutputParser().parse(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
